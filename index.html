<!DOCTYPE html>
<html lang="zh-CN">
 <head>
  <style type="text/css" id="26760718606"></style> 
  <title>VrR-VG - Visual-relevant Relationships</title> 
  <link rel="stylesheet" href="./dist/css/bootstrap.css" /> 
  <link rel="stylesheet" type="text/css" href="./dist/css/screen.css" /> 
  <style>
	#slideShowImages { /* The following CSS rules are optional. */
	  border: 1px gray solid;
	  background-color: lightgray;
	}	
  
	#slideShowImages img { /* The following CSS rules are optional. */
	  border: 0.8em black solid;
	  padding: 3px;
	}	
  </style>
 </head> 
 <body style="font-size: 16px;"> 
  <div class="header" style="background-repeat: round;height:320px; width:100%;background-image: url(./img/bg.jpg)"> 
   <h3>&nbsp;</h3> 
   <div class="container"> 
	<div class="row">
	 <div class="col-sm-12" style="margin-top: 40px;">
	  <h2 style="text-shadow: 0px 0px 10px #333;font-family:Helvetica; color:white">VrR-VG</h2> 
	  <br/>
	  <h5 style="text-shadow: 0px 0px 10px #333;font-family:verdana; color:white">Visual-relevant Relatinoships Dataset</h5> 
	 </div> 
	</div> 
   </div> 
  </div> 
  <nav class="navbar navbar-inverse navbar-fixed-top"> 
   <div class="container">
	<div class="navbar-header">

	 <a class="navbar-brand">VrR-VG</a>
	</div>
	<div id="navbar" class="collapse navbar-collapse">
	 <ul class="nav navbar-nav">
	  <li><a href="#02">Abstract</a></li>
	  <li><a href="#03">Details</a></li> 
	  <li><a href="#04">Download</a></li> 
	  <li><a href="#05">Contact us</a></li> 
	 </ul> 
	</div> 
	
   </div> 
  </nav> 
  <!-- start site's main content area --> 
  &nbsp;
  <section class="content-wrap">
   <div class="container"> 
	<div class="row"> 
	 <main class="col-md-9 main-content"> 
	  <article id="02" class="post">
	   <div align="justify"> 
		<h2 class="post-title">Abstract</h2> 
        <p style="font-family:Helvetica">   Relationships encode the interactions among individual instances and play a critical role in deep visual scene understanding. Suffering from the high predictability with non-visual information, relationship models tend to fit the statistical bias rather than ``learning" to infer the relationships from images. To encourage further development in visual relationships, we propose a novel method to mine more valuable relationships by automatically pruning visually-irrelevant relationships. We construct a new scene graph dataset named Visually-Relevant Relationships Dataset (VrR-VG) based on Visual Genome. Compared with existing datasets, the performance gap between learnable and statistical method is more significant in VrR-VG, and frequency-based analysis does not work anymore. Moreover, we propose to learn a relationship-aware representation by jointly considering instances, attributes and relationships. By applying the representation-aware feature learned on VrR-VG, the performances of image captioning and visual question answering are systematically improved, which demonstrates the effectiveness of both our dataset and features embedding schema. </p>
	   </div> 
	  </article> 
	  <article id="03" class="post"> 
	   <div align="justify"> 
		<h2 class="post-title">Details</h2> 
		<p>Visual-relevance Relationships dataset (VrR-VG) is a scene graph dataset from Visual Genome. It contains 117 visual-relevant relationships selected by our method.</p> 
		<p>VrR-VG is constructed from Visual Genome. The VrR-VG has 58983 images and 23375 relation pairs. Excluded positional and statistiaclly biased reltionships, the VrR-VG is more balance and includes more valuable relaiton data.Rather than generating scene graph dataset by relation label frequency, the VrR-VG gathers visual-relevant relationships, like "play with", "lay on", "hang on", etc. The relationships like "on" , "wearing", "has",  etc., which can be inferred by positional information or data bias, are excluded in our dataset. The VrR-VG offers a more balanced research material for relationships, and more diverse relation data for semantic tasks.</p>
		<p>Distribution comparison of datasets. The images in left column is from VG150, and the images in right is our VrR-VG. VrR-VG is more balance and diverse. </p>
		<img height="130px" width="750px" src="./img/distribution_cmp.jpg" /> 
        <p>Tag cloud comparison of dataset.</p>
		<table class="table table-borderless" align="center"> 
		<tbody> 
			<tr align="center"> 
			<td width="50%"> 
			<img height="200px" width="360px" src="./img/vg150_tag.jpg" />
			</td> 
			<td width="50%"> 
			<img height="200px" width="360px" src="./img/vrrvg_tag.jpg" />
			</td> 
			</tr> 
		</tbody> 
		</table> 
		<p>Scene graph generation examples for VrR-VG and previous VG150(splited by label frequency). The images in left column is from VG150, and the images in right is our VrR-VG. </p>
		<img height="400px" width="750px" src="./img/web_sg1.jpg" />
		<img height="400px" width="750px" src="./img/web_sg2.jpg" />
	   </div> 
	  </article> 
	  <article id="04" class="post"> 
	   <div align="justify"> 
		<h3 class="post-title">Download</h3> 
		<div> 
		 <table class="table"> 
		   <tr>
			<td width="50%">Download</td> 
			<td width="50%"> VrR-VG <a href="https://drive.google.com/open?id=15i5J87FHfZ1DwPkumBaH50nfOg01Ja8C">&nbsp;google drive</a></td>
		   </tr> 
		  </tbody> 
		 </table> 
		</div> 
	   </div> 
	  </article> 
	  <article id="05" class="post"> 
	   <div align="justify"> 
		<h2 class="post-title">Contact Us</h2> 
		<table class="table table-borderless" align="center"> 
		 <tbody> 
		  <tr align="center"> 
			<td width="25%"> 
			<br /> <a href="https://github.com/akira-l/VrR-VG" target="_blank">Yuanzhi Liang</a>
			<br /> Xi'an Jiaotong University
			<br /> JD AI Research
			<br /> liangyzh13@stu.xjtu.edu.cn
			</td> 

			<td width="25%"> 
			<br /> <a href="http://ylbai.asiteof.me" target="_blank">Yalong Bai</a>
			<br /> JD AI Research
			<br /> ylbai@outlook.com
			</td> 

		   <td width="25%"> 
		   <br /> <a href="https://wzhang34.github.io" target="_blank">Wei Zhang</a>
		   <br /> JD AI Research
		   <br /> wzhang.cu@gmail.com
		   </td> 

		   <td width="25%"> 
			<br /> <a href="https://taomei.me/" target="_blank">Tao Mei</a>
			<br /> JD AI Research
			<br /> tmei@live.com
			</td>
		  </tr> 
		 </tbody> 
		</table> 
	  </article> 
	  <!--
	  <article id="08" class="post">
	  <div align="justify">
	   <h2 class="post-title">Photo Gallary</h2>
	  </div> <br >
	   <div id="slideShowImages">
		<img width="100%" src="img/full/slide1-1.jpg" alt="Slide 1" />
		<img width="100%" src="img/full/slide2-1.jpg" alt="Slide 2" />
		<img width="100%" src="img/full/slide4-1.jpg" alt="Slide 3" />
	   </div>
	  </article>
	  -->
	 </main>
 
  
	 <aside class="col-md-3 sidebar"> 
	  <div class="widget"> 
	   <h3 class="title">News</h4> 
	   <div class="content community">
		<p ><b>Jan. 01, 2019:</b></p>
		<p class="small">VrR-VG website online.</p> 
	   </div> 
	  </div> 
	  <div class="widget"> 
	   <h4 class="title">About</h4> 
	   <div class="content community"> 
		<p class="small"> Visual-relevant Relationships dataset. For more details:
		<a href="https://arxiv.org/abs/1902.00313">Rethinking Visual Relationships for High-level Image Understanding. </a> </p> 
	   </div> 
	  </div> 
	 </aside> 
	</div> 
   </div> 
  </section> 
  <br >
  <footer class="main-footer"> 
   <div class="container"> 
	<div class="row"> 
	 <div class="col-sm-12"> 
	  <div class="widget" align="center"> 
	   <p> VrR-VG
	   <br /> 
	   Contact Us: liangyzh13@stu.xjtu.edu.cn
	   </p> 
	  </div> 
	 </div> 
	</div> 
   </div> 
  </footer>	 
 </body>
</html>
