<!DOCTYPE html>
<html lang="zh-CN">
 <head>
  <style type="text/css" id="26760718606"></style>  
  <title>VrR-VG - Visual-relevant Relationships</title> 
  <link rel="stylesheet" href="./dist/css/bootstrap.css" /> 
  <link rel="stylesheet" type="text/css" href="./dist/css/screen.css" /> 
  <style>
	#slideShowImages { /* The following CSS rules are optional. */
	  border: 1px gray solid;
	  background-color: lightgray;
	}	
  
	#slideShowImages img { /* The following CSS rules are optional. */
	  border: 0.8em black solid;
	  padding: 3px;
	}	
  </style>
 </head> 

 <body style="font-size: 16px;"> 

  <div class="header" style="background-repeat: round;height:320px; width:100%;background-image: url(./img/bg_sg_tiny.png)"> 
   <h3>&nbsp;</h3> 
   <div class="container"> 
	<div class="row">
	 <div class="col-sm-12" style="margin-top: 40px;">
	  <h2 style="text-shadow: 0px 0px 10px #333;font-family:Helvetica; color:white">VrR-VG : Visually-relevant Relationships</h2> 
	  <br/>
	  <h5 style="text-shadow: 0px 0px 10px #333;font-family:verdana; color:white"> What should be learned in visual relationships.</h5> 
	 </div> 
	</div> 
   </div> 
  </div> 
  <nav class="navbar navbar-inverse navbar-fixed-top"> 
   <div class="container">
	<div class="navbar-header">

	 <a class="navbar-brand">VrR-VG</a>
	</div>
	



	<div id="navbar" class="collapse navbar-collapse">
	 <ul class="nav navbar-nav">
	  <li><a href="#02">Abstract</a></li>
	  <li><a href="#03">Description</a></li> 
	  <li><a href="#04">Download</a></li> 
	  <li><a href="#05">Contact</a></li> 
	 </ul> 
	</div> 
	
   </div> 
  </nav> 
  <!-- start site's main content area --> 
  &nbsp;
  <section class="content-wrap">
   <div class="container"> 
	<div class="row"> 

	    <main class="col-md-9 main-content"> 
			<article id="02" class="post">
					<div align="justify"> 
					<h2 class="post-title"> Abstract </h2> 
					 <h4> Why we research visual relationships?  </h4>
					 <p>Visual relationships connect isolated instances into the structural graph. It provides a dimension in scene understanding, which is higher than the single instance and lower than the holistic scene. The visual relationships act as the bridge of perception and cognition. </p>

					 <h4><p style="font-family:Helvetica"> What have we done in visual relationships?  </p> </h4>
					 <p>From the phrase detection to the scene graph generation, we have a clearer data standard and task for representing relationships. </p>

					 <h4> <p style="font-family:Helvetica"> What next to visual relationships? </p> </h4>
					 <p>After the representation of visual relationships, we should use the relationship information and build the bridge from perception to cognition. More than given the correct scene graph, the relationships should go further in the semantic and play the actual role in scene understanding.</p>

					 <h4><p style="font-family:Helvetica"> Why the applications in visual relationships stuck?   </p> </h4>
					 <p>Rather than the method side, more problems exist in the data side. Before designing methods on how to learn, we should figure out what to learn first.  </p>

					 <h4><p style="font-family:Helvetica"> What should be learned in visual relationships for cognition?  </p> </h4>
					 <p> <strong>Visually-relevant relationships!</strong> The visually-irrelevant relationships like spatial relationships and low diversity relationships degrade the relation problems to detection or deductive reasoning. Those visually-irrelevant relationships pull back the relationships inferring to the perceptive side. To take advantage of relationship information for semantic understanding, Only the Visually-relevant relationships should be learned!</p>
					</div> 
				   </article> 
	   <!-- <article id="02" class="post">
	    <div align="justify"> 
		<h2 class="post-title">Abstract</h2> 
        <p style="font-family:Helvetica">   Relationships encode the interactions among individual instances and play a critical role in deep visual scene understanding. Suffering from the high predictability with non-visual information, relationship models tend to fit the statistical bias rather than 'learning' to infer the relationships from images. To encourage further development in visual relationships, we propose a novel method to mine more valuable relationships by automatically pruning visually-irrelevant relationships. We construct a new scene graph dataset named Visually-Relevant Relationships Dataset (VrR-VG) based on Visual Genome. Compared with existing datasets, the performance gap between learnable and statistical method is more significant in VrR-VG, and frequency-based analysis does not work anymore. Moreover, we propose to learn a relationship-aware representation by jointly considering instances, attributes and relationships. By applying the representation-aware feature learned on VrR-VG, the performances of image captioning and visual question answering are systematically improved, which demonstrates the effectiveness of both our dataset and features embedding schema. </p>
	   </div> 
	  </article>  --> 
	 	
	  <article id="03" class="post"> 
	   <div align="justify"> 
		<h2 class="post-title">Description</h2> 
		<p>Visual-relevance Relationships dataset (VrR-VG) is a scene graph dataset from Visual Genome. It contains 117 visual-relevant relationships selected by our method.</p> 
		<p>VrR-VG is constructed from Visual Genome. The VrR-VG has 58983 images and 23375 relation pairs. Excluded positional and statistically biased relationships, the VrR-VG is more balance and includes more valuable relation data. Rather than generating scene graph dataset by relation label frequency, the VrR-VG gathers visual-relevant relationships, like "play with", "lay on", "hang on", etc. The relationships like "on", "wearing", "has", etc., which can be inferred by positional information or data bias, are excluded in our dataset. The VrR-VG offers a more balanced research material for relationships and more diverse relation data for semantic tasks.</p>
		<p>Distribution comparison of datasets. The images in the left column are from VG150, and the images in right are our VrR-VG. VrR-VG is more balanced and diversity. </p>
		<img height="130px" width="750px" src="./img/distribution_cmp.jpg" /> 
		<p> </p>
        <p>Tag cloud comparison of dataset.</p>
		<table class="table table-borderless" align="center"> 
		<tbody> 
			<tr align="center"> 
			<td width="50%"> 
			<img height="200px" width="360px" src="./img/vg150_tag.jpg" />
			</td> 
			<td width="50%"> 
			<img height="200px" width="360px" src="./img/vrrvg_tag.jpg" />
			</td> 
			</tr> 
		</tbody> 
		</table> 
		<!-- <p>Scene graph generation examples for VrR-VG and previous VG150(splited by label frequency). The images in left column is from VG150, and the images in right is our VrR-VG. </p>
		<img height="400px" width="750px" src="./img/web_sg1.jpg" />
		<img height="400px" width="750px" src="./img/web_sg2.jpg" />
		-->
	   </div> 
	  </article> 
	  <article id="04" class="post"> 
	   <div align="justify"> 
		<h3 class="post-title">Download</h3> 
		<div> 
		 <table class="table"> 
		   <tr>
			<td width="40%">Download</td> 
			<td width="30%"> VrR-VG <a href="https://drive.google.com/open?id=15i5J87FHfZ1DwPkumBaH50nfOg01Ja8C">&nbsp;google drive</a></td>
			<td width="30%"> VD-Net <a href="https://github.com/vrrvg/vrrvg.github.io/blob/master/VD-Net.py">&nbsp; github </a></td>
		   </tr> 
		  </tbody> 
		  <p> For scene graph generation, you can take the last 5000 samples as the test set (the same setting in <a href="https://github.com/rowanz/neural-motifs/blob/master/dataloaders/visual_genome.py">neural-motifs</a>). For features representation, you can use all the samples for training. </p>
		  <p> An implemented example for VD-Net is also provided. For choosing our complete visually-relevant relationships, multiple initial learning rates (learning from 1e-5 to 1e-2, momentum is 0.9, weight decay is 1e-4) were used and we took 10 models for voting. Additionally, to avoid overfitting, the data sample should be completely random. In each batch, the relation triplets were from different images and the labels should be different from each other. </p>
		 </table> 
		</div> 

	   </div> 
	  </article> 

	  <article id="05" class="post"> 
	   <div align="justify"> 
		<h2 class="post-title">Contact</h2> 
		<table class="table table-borderless" align="center"> 
		 <tbody> 
		  <tr align="center"> 
			<td width="25%"> 
			<br /> <a href="http://liangyzh.com/" target="_blank">Yuanzhi Liang</a>
			<br /> Xi'an Jiaotong University
			<br /> JD AI Research
			<br /> liangyzh13@stu.xjtu.edu.cn 
			</td> 

			<td width="25%"> 
			<br /> <a href="http://ylbai.asiteof.me" target="_blank">Yalong Bai</a>
			<br /> JD AI Research
			<br /> ylbai@outlook.com
			</td> 

		   <td width="25%"> 
		   <br /> <a href="https://wzhang34.github.io" target="_blank">Wei Zhang</a>
		   <br /> JD AI Research
		   <br /> wzhang.cu@gmail.com
		   </td> 

		   <td width="25%"> 
			<br /> <a href="https://taomei.me/" target="_blank">Tao Mei</a>
			<br /> JD AI Research
			<br /> tmei@live.com
			</td>
		  </tr> 
		 </tbody> 
		</table> 
	  </article> 

	  <div align="center"> 
	  <a href="https://clustrmaps.com/site/1b5wb"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=Er963A9qfDcLEdSL9Df1Qj4eugA6sgR7r7emvSvZdjs&cl=ffffff" /></a>
	  </div>

	  <!--
	  <article id="08" class="post">
	  <div align="justify">
	   <h2 class="post-title">Photo Gallary</h2>
	  </div> <br >
	   <div id="slideShowImages">
		<img width="100%" src="img/full/slide1-1.jpg" alt="Slide 1" />
		<img width="100%" src="img/full/slide2-1.jpg" alt="Slide 2" />
		<img width="100%" src="img/full/slide4-1.jpg" alt="Slide 3" />
	   </div>
	  </article>
	  -->
	 </main>
 
  
	 <aside class="col-md-3 sidebar"> 
	  <div class="widget"> 
	   <h3 class="title">News</h4> 
	   <div class="content community">
		<p ><b>Jan. 01, 2019:</b></p>
		<p class="small">VrR-VG website online.</p> 
		<p ><b>Jul. 23, 2019:</b></p>
		<p class="small">Accepted in ICCV2019.</p> 
	   </div> 
	  </div> 
	  <div class="widget"> 
	   <h4 class="title">Paper</h4> 
	   <div class="content community"> 
		<p class="small"> VrR-VG arxiv version update. </p>
		<p>	<a href="https://arxiv.org/abs/1902.00313">VrR-VG: Refocusing Visually-Relevant Relationships. </a> </p> 
		<p class="small"> ICCV version update. </p>	
		<p>	<a href="http://202.117.4.101/cache/12/03/openaccess.thecvf.com/47beef6649903f94f9e6545d7de35f64/Liang_VrR-VG_Refocusing_Visually-Relevant_Relationships_ICCV_2019_paper.pdf">VrR-VG: Refocusing Visually-Relevant Relationships. </a> </p> 	
	   </div> 
	  </div> 
	 </aside> 
	</div> 
   </div> 
  </section> 
  <br >
  <footer class="main-footer"> 
   <div class="container"> 
	<div class="row"> 
	 <div class="col-sm-12"> 
	  <div class="widget" align="center"> 
	   <p> VrR-VG
	   <br /> 
	   Contact Us: liangyzh13@stu.xjtu.edu.cn
	   </p> 
	  </div> 
	 </div> 
	</div> 
   </div> 
  </footer>	 
 </body>
</html>
