<!DOCTYPE html>
<html lang="zh-CN">
 <head>
  <style type="text/css" id="26760718606"></style> 
  <title>VrR-VG - Visual-relevant Relationships</title> 
  <link rel="stylesheet" href="./dist/css/bootstrap.css" /> 
  <link rel="stylesheet" type="text/css" href="./dist/css/screen.css" /> 
  <style>
	#slideShowImages { /* The following CSS rules are optional. */
	  border: 1px gray solid;
	  background-color: lightgray;
	}	
  
	#slideShowImages img { /* The following CSS rules are optional. */
	  border: 0.8em black solid;
	  padding: 3px;
	}	
  </style>
 </head> 
 <body style="font-size: 16px;"> 
  <div class="header" style="background-repeat: round;height:320px; width:100%;background-image: url(./img/bg.jpg)"> 
   <h3>&nbsp;</h3> 
   <div class="container"> 
	<div class="row">
	 <div class="col-sm-12" style="margin-top: 40px;">
	  <h2 style="text-shadow: 0px 0px 10px #333;font-family:Helvetica; color:white">VrR-VG</h2> 
	  <br/>
	  <h5 style="text-shadow: 0px 0px 10px #333;font-family:verdana; color:white">Visually-relevant Relationships</h5> 
	 </div> 
	</div> 
   </div> 
  </div> 
  <nav class="navbar navbar-inverse navbar-fixed-top"> 
   <div class="container">
	<div class="navbar-header">

	 <a class="navbar-brand">VrR-VG</a>
	</div>
	<div id="navbar" class="collapse navbar-collapse">
	 <ul class="nav navbar-nav">
	  <li><a href="#02">Abstract</a></li>
	  <li><a href="#03">Details</a></li> 
	  <li><a href="#04">Download</a></li> 
	  <li><a href="#05">Contact us</a></li> 
	 </ul> 
	</div> 
	
   </div> 
  </nav> 
  <!-- start site's main content area --> 
  &nbsp;
  <section class="content-wrap">
   <div class="container"> 
	<div class="row"> 

	    <main class="col-md-9 main-content"> 
			<article id="02" class="post">
					<div align="justify"> 
					 <h4> Why we research on visual relationships?  </h4>
					 <p>Visual relationships connect isolated instances into the structual graph. It provides a dimension in scene understanding, which is higher than single instance and lower than the holistic scene. The visual relationships act as the bridge of perception and cognition. </p>

					 <h4><p style="font-family:Helvetica"> What we have done in visual relationships?  </p> </h4>
					 <p>From the phrase detection to the scene graph generation, we have the clearer data standard and task for representing relationships. </p>

					 <h4> <p style="font-family:Helvetica"> What next to visual relationships?  </p> </h4>
					 <p>After representation of visual relationships, we should use the relationships information and build the bridge from perception to cognition. More than given correct scene graph, the relationships should go further in the semantic and play the actual role in scene understanding.</p>

					 <h4><p style="font-family:Helvetica"> Why the applications in visual relationships stuck?  </p> </h4>
					 <p>Rather than the method side, more problems exist in data side. Before designing methods in how to learn, we should figure out what to learn first. </p>

					 <h4><p style="font-family:Helvetica"> What should be learned in visual relationships for cognition?  </p> </h4>
					 <p> <strong>Visually-relevant relationships!</strong> The visually-irrelevant relationships like spatial relationships and low diversity relationships degrade the relation problems to detection or deductive reasoning. Those visually-irrelevant relationships pull back the relationships inferring to perceptive side. To take advantage of relationships information for semantic understanding, Only the Visually-relevant relationships should be learned!</p>
					</div> 
				   </article> 
	    <article id="02" class="post">

	   <div align="justify"> 
		<h2 class="post-title">Abstract</h2> 
        <p style="font-family:Helvetica">   Relationships encode the interactions among individual instances and play a critical role in deep visual scene understanding. Suffering from the high predictability with non-visual information, relationship models tend to fit the statistical bias rather than 'learning' to infer the relationships from images. To encourage further development in visual relationships, we propose a novel method to mine more valuable relationships by automatically pruning visually-irrelevant relationships. We construct a new scene graph dataset named Visually-Relevant Relationships Dataset (VrR-VG) based on Visual Genome. Compared with existing datasets, the performance gap between learnable and statistical method is more significant in VrR-VG, and frequency-based analysis does not work anymore. Moreover, we propose to learn a relationship-aware representation by jointly considering instances, attributes and relationships. By applying the representation-aware feature learned on VrR-VG, the performances of image captioning and visual question answering are systematically improved, which demonstrates the effectiveness of both our dataset and features embedding schema. </p>
	   </div> 
	  </article> 
	  <article id="03" class="post"> 
	   <div align="justify"> 
		<h2 class="post-title">Details</h2> 
		<p>Visual-relevance Relationships dataset (VrR-VG) is a scene graph dataset from Visual Genome. It contains 117 visual-relevant relationships selected by our method.</p> 
		<p>VrR-VG is constructed from Visual Genome. The VrR-VG has 58983 images and 23375 relation pairs. Excluded positional and statistiaclly biased reltionships, the VrR-VG is more balance and includes more valuable relaiton data.Rather than generating scene graph dataset by relation label frequency, the VrR-VG gathers visual-relevant relationships, like "play with", "lay on", "hang on", etc. The relationships like "on" , "wearing", "has",  etc., which can be inferred by positional information or data bias, are excluded in our dataset. The VrR-VG offers a more balanced research material for relationships, and more diverse relation data for semantic tasks.</p>
		<p>Distribution comparison of datasets. The images in left column is from VG150, and the images in right is our VrR-VG. VrR-VG is more balance and diverse. </p>
		<img height="130px" width="750px" src="./img/distribution_cmp.jpg" /> 
        <p>Tag cloud comparison of dataset.</p>
		<table class="table table-borderless" align="center"> 
		<tbody> 
			<tr align="center"> 
			<td width="50%"> 
			<img height="200px" width="360px" src="./img/vg150_tag.jpg" />
			</td> 
			<td width="50%"> 
			<img height="200px" width="360px" src="./img/vrrvg_tag.jpg" />
			</td> 
			</tr> 
		</tbody> 
		</table> 
		<p>Scene graph generation examples for VrR-VG and previous VG150(splited by label frequency). The images in left column is from VG150, and the images in right is our VrR-VG. </p>
		<img height="400px" width="750px" src="./img/web_sg1.jpg" />
		<img height="400px" width="750px" src="./img/web_sg2.jpg" />
	   </div> 
	  </article> 
	  <article id="04" class="post"> 
	   <div align="justify"> 
		<h3 class="post-title">Download</h3> 
		<div> 
		 <table class="table"> 
		   <tr>
			<td width="50%">Download</td> 
			<td width="50%"> VrR-VG <a href="https://drive.google.com/open?id=15i5J87FHfZ1DwPkumBaH50nfOg01Ja8C">&nbsp;google drive</a></td>
		   </tr> 
		  </tbody> 
		 </table> 
		</div> 
	   </div> 
	  </article> 
	  <article id="05" class="post"> 
	   <div align="justify"> 
		<h2 class="post-title">Contact Us</h2> 
		<table class="table table-borderless" align="center"> 
		 <tbody> 
		  <tr align="center"> 
			<td width="25%"> 
			<br /> <a href="https://github.com/akira-l/VrR-VG" target="_blank">Yuanzhi Liang</a>
			<br /> Xi'an Jiaotong University
			<br /> JD AI Research
			<br /> liangyzh13@stu.xjtu.edu.cn 
			</td> 

			<td width="25%"> 
			<br /> <a href="http://ylbai.asiteof.me" target="_blank">Yalong Bai</a>
			<br /> JD AI Research
			<br /> ylbai@outlook.com
			</td> 

		   <td width="25%"> 
		   <br /> <a href="https://wzhang34.github.io" target="_blank">Wei Zhang</a>
		   <br /> JD AI Research
		   <br /> wzhang.cu@gmail.com
		   </td> 

		   <td width="25%"> 
			<br /> <a href="https://taomei.me/" target="_blank">Tao Mei</a>
			<br /> JD AI Research
			<br /> tmei@live.com
			</td>
		  </tr> 
		 </tbody> 
		</table> 
	  </article> 
	  <!--
	  <article id="08" class="post">
	  <div align="justify">
	   <h2 class="post-title">Photo Gallary</h2>
	  </div> <br >
	   <div id="slideShowImages">
		<img width="100%" src="img/full/slide1-1.jpg" alt="Slide 1" />
		<img width="100%" src="img/full/slide2-1.jpg" alt="Slide 2" />
		<img width="100%" src="img/full/slide4-1.jpg" alt="Slide 3" />
	   </div>
	  </article>
	  -->
	 </main>
 
  
	 <aside class="col-md-3 sidebar"> 
	  <div class="widget"> 
	   <h3 class="title">News</h4> 
	   <div class="content community">
		<p ><b>Jan. 01, 2019:</b></p>
		<p class="small">VrR-VG website online.</p> 
		<p ><b>Jul. 23, 2019:</b></p>
		<p class="small">Accepted in ICCV2019.</p> 
	   </div> 
	  </div> 
	  <div class="widget"> 
	   <h4 class="title">About</h4> 
	   <div class="content community"> 
		<p class="small"> Visual-relevant Relationships dataset. Paper and dataset released. 
		<a href="https://arxiv.org/abs/1902.00313">Rethinking Visual Relationships for High-level Image Understanding. </a> </p> 
		<p class="small"> VrR-VG paper update. 
			<a href="https://arxiv.org/abs/1902.00313">VrR-VG: Refocusing Visually-Relevant Relationships. </a> </p> 
	   </div> 
	  </div> 
	 </aside> 
	</div> 
   </div> 
  </section> 
  <br >
  <footer class="main-footer"> 
   <div class="container"> 
	<div class="row"> 
	 <div class="col-sm-12"> 
	  <div class="widget" align="center"> 
	   <p> VrR-VG
	   <br /> 
	   Contact Us: liangyzh13@stu.xjtu.edu.cn
	   </p> 
	  </div> 
	 </div> 
	</div> 
   </div> 
  </footer>	 
 </body>
</html>
